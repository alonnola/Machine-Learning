table(ab6190.sub$ECOSYS)
# Problem here: because we are doing subsetting, we end up with empty classes.
# Remove rare groups (n<10) from data:
ab6190.sub.rm <- ab6190.sub %>% group_by(ECOSYS) %>% filter(n() >= 20)
# expunge empty levels from factor ECOSYS
ab6190.sub.rm$ECOSYS=factor(ab6190.sub.rm$ECOSYS)
# classify the ECOSYS category based on climate data
ecosys=read.csv("E:\\ENVS 316\\R stuff\\Machine learning\\AB_Ecosystems.csv")
ecol=as.character(ecosys$ECOL)
ename=ecosys$ENAME
plot(Y~X, pch=21, col=ecol[ECOSYS],data=ab6190.sub.rm,main="Actual 61-90")
# build SVM model using default parameters, and predict classes
svm_model <- svm(ECOSYS~.,data=ab6190.sub.rm) #MAT+MWMT+MCMT+MAP+MSP+AHM
summary(svm_model)
# run predictions on the training data, to get MCR
pred <- predict(svm_model,newdata=ab6190.sub.rm)
confusionMatrix(ab6190.sub.rm$ECOSYS,pred)$overall
# test accuracy via the missclassification rate (MCR)
MCR=1-sum(diag(prop.table(table(ab6190.sub.rm$ECOSYS,pred))))
# verify classifications, and the model fit
# chi-sq test for overall significance of predicted classes
chisq.test(ab6190.sub.rm$ECOSYS,pred)
#
# Is this SVM model the best we can do? Use tune() to select optimum parameters
C=10^(0:2)
G=2^seq(0,4,1)
ncv<-10 # number of cross-validation folds
# tune SVM using train() under caret
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(ECOSYS~.,data=ab6190.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = 9,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
# same thing using tune()
system.time(svm_tune <- tune(svm,ECOSYS~.,data=ab6190.sub.rm,kernel="radial", ranges=list(cost=C, gamma=G),tunecontrol = tune.control(sampling = "cross",cross=ncv)))
print(svm_tune)
plot(svm_tune)
svm_fit
plot(svm_fit)
View(ab6190.sub.rm)
svm_model <- svm(ECOSYS~.,data=ab6190.sub.rm[,-1]) #MAT+MWMT+MCMT+MAP+MSP+AHM
summary(svm_model)
pred <- predict(svm_model,newdata=ab6190.sub.rm)
confusionMatrix(ab6190.sub.rm$ECOSYS,pred)$overall
MCR=1-sum(diag(prop.table(table(ab6190.sub.rm$ECOSYS,pred))))
chisq.test(ab6190.sub.rm$ECOSYS,pred)
C=10^(0:2)
G=2^seq(0,4,1)
ncv<-10 # number of cross-validation folds
# tune SVM using train() under caret
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(ECOSYS~.,data=ab6190.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = 9,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
2^seq(0,4,1)
G=2^seq(0,.4,1)
G
G=2^seq(0,4,1)
G
seq(0,4,1)
C=10^(0:2)
G=c(0,0.25,.5,1)
ncv<-10 # number of cross-validation folds
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(ECOSYS~.,data=ab6190.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = 9,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
library(MASS)
### change the following line to point to your CSV file:
filename<-"E://ENVS 316//assignment 4//covtype_sm_sample.csv"
# read the data and pre-process
trees=read.csv(filename,row.names = 1)
# make sure tree Type and Wilderness Area are properly coded as multi-level factors:
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1
trees$Area2[trees$Area2==1]=2
trees$Area3[trees$Area3==1]=3
trees$Area4[trees$Area4==1]=4
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
# remove the four AreaN columns:
trees[,11:14]<-NULL
#
# scale the data (numeric variables only)
trees[,1:10]=scale(trees[,1:10])
#
# 1. Divide data into training (80%) and test (20%) by doing random sample without replacement
set.seed(10101)
# Now Selecting 80% of data as sample from total 'n' rows of the data
sample <- sample.int(n = nrow(trees), size = floor(.80*nrow(trees)), replace = F)
trees_train <- trees[sample, ]
trees_test  <- trees[-sample, ]# these are the training set subscripts
# 2. Build LDA model on scaled training data
# first use all numeric predictors (i.e. not the factor Area)
trees.lda<-lda(Type ~ Elevation + Aspect + Slope + HDistHydrol + VDistHydrol + HDistRoad + Hillshade9a + Hillshade12p + Hillshade3p + HDistFire, data = trees_train, CV = T )
trees.lda2<-lda(Type ~ Elevation +  VDistHydrol + Hillshade9a , data = trees_train, CV = T )
# test accuracy via the missclassification rate (MCR)
trees.MCR = 1 - sum(diag(prop.table(table(trees_train[,11],trees.lda2$class))))
trees.MCR
# chi-sq test for overall significance of predicted classes
chisq.test(trees_train[,11],trees.lda$class)
#interpret seperation
# use MANOVA to get Wilks test result:
summary(manova(as.matrix(trees_train[,1:10])~trees_train[,11]),test="Wilks")
# and summary.aov() to get individual contributions ?
summary.aov(manova(as.matrix(trees_train[,1:10])~trees_train[,11]),test="Wilks")
# fit the model again without CV for prediction later
trees.lda<-lda(Type ~ Elevation + Aspect + Slope + HDistHydrol + VDistHydrol + HDistRoad + Hillshade9a + Hillshade12p + Hillshade3p + HDistFire, data = trees_train, CV = F )
# summarise model
trees.lda
# 3. Model specification and testing:
barplot(trees.lda$means,beside=T)
# determine which LD components are important using barplot
plot(trees.lda,dimen=1,type="both")
trees.lda.1.pred=predict(trees.lda,trees_train)
trees.lda.1.pred$x
# plot LDA 1 vs 2 for actual classes vs predicted (stored in an lda() model object called lda.1.pred)
?plot
par(mfrow = c(2, 1))
plot(trees.lda.1.pred$x[,1],trees.lda.1.pred$x[,2],col=trees_train$Type,main="(a) Actual")
legend(-4.2,-2.5,legend=seq(1,7,1),col=seq(1,7,1),pch=20,horiz = T)
plot(trees.lda.1.pred$x[,1],trees.lda.1.pred$x[,2],col=trees.lda.1.pred$class,main="(b) Predicted")
legend(-4.2,-6,legend=seq(1,7,1),col=seq(1,7,1),pch=20,horiz = T)
# return plotting page to normal
layout(1)
# 4. Prediction of test data:
trees.lda.pred<-predict(trees.lda,trees_test)
# apply full model to test data and get MCR:
(trees.testMCR <- 1 - sum(diag(prop.table(table(trees_test[,11],trees.lda.pred$class)))))
# Clustering: find out how many distinct tree types we really have...
#
# tree diagram (work on a random sample of n=1000 to speed things up):
sam=sample(seq(1,80000,1),size=1000)
hc = hclust(dist(trees_train[sam,1:10]))
hcd=as.dendrogram(hc)
plot(hcd)
# very simple dendrogram, cut at h=10
plot(cut(hcd, h = 10)$upper, main = "Upper tree of cut at h=10")
# use EH Ch 9 method for determining how many clusters based on iterative within groups sum of squares
wss <- (nrow(trees_train[,1:10])-1)*sum(apply(trees_train[,1:10],2,var))
for (i in 2:15) wss[i] <- sum(kmeans(trees_train[,1:10],centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# k-means fit with k = ?
k = 6
fit <- kmeans(trees_train[,1:10],k)
# Centroid Plot against 1st 2 discriminant functions (explain 95%+ variations)
library(fpc)
plotcluster(trees_train[,1:10],fit$cluster)
trees_train[,11]
View(trees_train)
ab6190=read.csv("E:\\ENVS 316\\R stuff\\Machine learning\\AB_Climate.csv",row.names = 1)
ab6190=read.csv("E:\\ENVS 316\\R stuff\\Machine learning\\AB_Climate.csv",row.names = 0)
View(ab6190)
View(ab6190)
ab6190=read.csv("E:\\ENVS 316\\R stuff\\Machine learning\\AB_Climate.csv",row.names = 1)
View(ab6190)
View(ab6190)
ab6190=read.csv("E:\\ENVS 316\\R stuff\\Machine learning\\AB_Climate.csv",row.names = 1)
View(ab6190)
View(ab6190)
C=10^(-1:5)
G=2^seq(-15,3,2)
ncv<-10 # number of cross-validation folds
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
chisq.test(trees_train[,11],trees.lda$class)
chisq.test(trees_train[,11],trees.lda$class)
# A4: Classification based on Roosevelt Forest Trees dataset
library(MASS)
### change the following line to point to your CSV file:
filename<-"E://ENVS 316//assignment 4//covtype_sm_sample.csv"
# read the data and pre-process
trees=read.csv(filename,row.names = 1)
# make sure tree Type and Wilderness Area are properly coded as multi-level factors:
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1
trees$Area2[trees$Area2==1]=2
trees$Area3[trees$Area3==1]=3
trees$Area4[trees$Area4==1]=4
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
# remove the four AreaN columns:
trees[,11:14]<-NULL
#
# scale the data (numeric variables only)
trees[,1:10]=scale(trees[,1:10])
#
# 1. Divide data into training (80%) and test (20%) by doing random sample without replacement
set.seed(10101)
# Now Selecting 80% of data as sample from total 'n' rows of the data
sample <- sample.int(n = nrow(trees), size = floor(.80*nrow(trees)), replace = F)
trees_train <- trees[sample, ]
trees_test  <- trees[-sample, ]# these are the training set subscripts
# 2. Build LDA model on scaled training data
# first use all numeric predictors (i.e. not the factor Area)
trees.lda<-lda(Type ~ Elevation + Aspect + Slope + HDistHydrol + VDistHydrol + HDistRoad + Hillshade9a + Hillshade12p + Hillshade3p + HDistFire, data = trees_train, CV = T )
trees.lda2<-lda(Type ~ Elevation +  VDistHydrol + Hillshade9a , data = trees_train, CV = T )
# test accuracy via the missclassification rate (MCR)
trees.MCR = 1 - sum(diag(prop.table(table(trees_train[,11],trees.lda2$class))))
trees.MCR
# chi-sq test for overall significance of predicted classes
chisq.test(trees_train[,11],trees.lda$class)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
library(dplyr)
library(e1071)
library(MASS)
# envoke RF library
library(randomForest)
# Regression/classification tree library
library(rpart)
# plotting trees
library(rpart.plot)
# enable caret for CV
library(caret)
# set a random seed for reproducbility (not during CV though...)
set.seed(321)
### change the following line to point to your CSV file:
trees=read.csv("E:\\ENVS 316\\assignment 5\\covtype_sm_sample.csv",row.names=1)
# correct factor variables:
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1 #as.factor(trees$Area1)
trees$Area2[trees$Area2==1]=2 #as.factor(trees$Area1)
trees$Area3[trees$Area3==1]=3 #as.factor(trees$Area1)
trees$Area4[trees$Area4==1]=4 #as.factor(trees$Area1)
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
trees[,11:14]<-NULL
# work with a random sample subset of the data to speed things up:
trees.sub <- trees[sample(1:nrow(trees), 1000, replace=FALSE),]
# Problem here: because we are doing subsetting, we end up with empty classes.
# Remove rare groups (n<10) from data:
trees.sub.rm <- trees.sub %>% group_by(Type) %>% filter(n() >= 10)
# expunge empty levels from factor Type
trees.sub.rm$Type=factor(trees.sub.rm$Type)
View(trees.sub.rm)
C=10^(-1:5)
G=2^seq(-15,3,2)
ncv<-10 # number of cross-validation folds
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
# we need dplyr and e1071 for SVM/RF
library(dplyr)
library(e1071)
library(MASS)
# envoke RF library
library(randomForest)
# Regression/classification tree library
library(rpart)
# plotting trees
library(rpart.plot)
# enable caret for CV
library(caret)
svm_fit
# we need dplyr and e1071 for SVM/RF
library(dplyr)
library(e1071)
library(MASS)
# envoke RF library
library(randomForest)
# Regression/classification tree library
library(rpart)
# plotting trees
library(rpart.plot)
# enable caret for CV
library(caret)
# set a random seed for reproducbility (not during CV though...)
set.seed(321)
### change the following line to point to your CSV file:
trees=read.csv("E:\\ENVS 316\\assignment 5\\covtype_sm_sample.csv",row.names=1)
# correct factor variables:
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1 #as.factor(trees$Area1)
trees$Area2[trees$Area2==1]=2 #as.factor(trees$Area1)
trees$Area3[trees$Area3==1]=3 #as.factor(trees$Area1)
trees$Area4[trees$Area4==1]=4 #as.factor(trees$Area1)
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
trees[,11:14]<-NULL
# work with a random sample subset of the data to speed things up:
trees.sub <- trees[sample(1:nrow(trees), 1000, replace=FALSE),]
# Problem here: because we are doing subsetting, we end up with empty classes.
# Remove rare groups (n<10) from data:
trees.sub.rm <- trees.sub %>% group_by(Type) %>% filter(n() >= 10)
# expunge empty levels from factor Type
trees.sub.rm$Type=factor(trees.sub.rm$Type)
View(trees.sub.rm)
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
### Students need to start editing the script here ####
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
#  fit RF to Type on all predictor vars:
# ntree = number of decision trees to grown, mtry = number of subset variables per split
fit <- randomForest(Type ~ .,data = trees.sub.rm, ntree = 5000,mtry = 2)
# make predictions of Group and test accuracy (*not* cross-validated)
pred=predict(fit,newdata=trees.sub.rm,type="class")
confusionMatrix(trees.sub.rm$Type,pred)$overall
#out of bag error
fit
#using train function from caret
train_control <- trainControl(method = "repeatedcv",number=10,repeats=2)
tune_grid = expand.grid(mtry=c(3,4,5))
cv_fit <- train(Type ~.,data = trees.sub.rm,method="rf",trControl=train_control,tuneGrid=tune_grid,ntree=5000)
cv_fit
plot(varImp(cv_fit))
#SVM
C=10^(-1:5)
G=2^seq(-15,3,2)
ncv<-10 # number of cross-validation folds
# tune SVM using train() under caret
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
?randomForest
?seq
?^
2^1
2^2
G=2^seq(3,9,2)?seq
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
plot(svm_fit)
?randomForest
# we need dplyr and e1071 for SVM/RF
library(dplyr)
library(e1071)
library(MASS)
# envoke RF library
library(randomForest)
# Regression/classification tree library
library(rpart)
# plotting trees
library(rpart.plot)
# enable caret for CV
library(caret)
# set a random seed for reproducbility (not during CV though...)
set.seed(321)
### change the following line to point to your CSV file:
trees=read.csv("E:\\ENVS 316\\assignment 5\\covtype_sm_sample.csv",row.names=1)
# correct factor variables:
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1 #as.factor(trees$Area1)
trees$Area2[trees$Area2==1]=2 #as.factor(trees$Area1)
trees$Area3[trees$Area3==1]=3 #as.factor(trees$Area1)
trees$Area4[trees$Area4==1]=4 #as.factor(trees$Area1)
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
trees[,11:14]<-NULL
# work with a random sample subset of the data to speed things up:
trees.sub <- trees[sample(1:nrow(trees), 1000, replace=FALSE),]
# Problem here: because we are doing subsetting, we end up with empty classes.
# Remove rare groups (n<10) from data:
trees.sub.rm <- trees.sub %>% group_by(Type) %>% filter(n() >= 10)
# expunge empty levels from factor Type
trees.sub.rm$Type=factor(trees.sub.rm$Type)
View(trees.sub.rm)
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
### Students need to start editing the script here ####
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
#  fit RF to Type on all predictor vars:
# ntree = number of decision trees to grown, mtry = number of subset variables per split
fit <- randomForest(Type ~ .,data = trees.sub.rm, ntree = 5000,mtry = 2)
# make predictions of Group and test accuracy (*not* cross-validated)
pred=predict(fit,newdata=trees.sub.rm,type="class")
confusionMatrix(trees.sub.rm$Type,pred)$overall
#out of bag error
fit
#using train function from caret
train_control <- trainControl(method = "repeatedcv",number=10,repeats=2)
tune_grid = expand.grid(mtry=c(3,4,5))
cv_fit <- train(Type ~.,data = trees.sub.rm,method="rf",trControl=train_control,tuneGrid=tune_grid,ntree=5000)
C=10^(-1:5)
G=2^seq(-15,3,2)
ncv<-10 # number of cross-validation folds
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
?randomForest.default
# we need dplyr and e1071 for SVM/RF
library(dplyr)
library(e1071)
library(MASS)
# envoke RF library
library(randomForest)
# Regression/classification tree library
library(rpart)
# plotting trees
library(rpart.plot)
# enable caret for CV
library(caret)
set.seed(321)
trees=read.csv("E:\\ENVS 316\\assignment 5\\covtype_sm_sample.csv",row.names=1)
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1 #as.factor(trees$Area1)
trees$Area2[trees$Area2==1]=2 #as.factor(trees$Area1)
trees$Area3[trees$Area3==1]=3 #as.factor(trees$Area1)
trees$Area4[trees$Area4==1]=4 #as.factor(trees$Area1)
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
trees[,11:14]<-NULL
trees.sub <- trees[sample(1:nrow(trees), 1000, replace=FALSE),]
trees.sub.rm <- trees.sub %>% group_by(Type) %>% filter(n() >= 10)
trees.sub.rm$Type=factor(trees.sub.rm$Type)
View(trees.sub.rm)
C=10^(-1:5)
G=2^seq(-15,3,2)
ncv<-10 # number of cross-validation folds
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
# we need dplyr and e1071 for SVM/RF
library(dplyr)
library(e1071)
library(MASS)
# envoke RF library
library(randomForest)
# Regression/classification tree library
library(rpart)
# plotting trees
library(rpart.plot)
# enable caret for CV
library(caret)
# set a random seed for reproducbility (not during CV though...)
set.seed(321)
### change the following line to point to your CSV file:
trees=read.csv("E:\\ENVS 316\\assignment 5\\covtype_sm_sample.csv",row.names=1)
# correct factor variables:
trees$Type=as.factor(trees$Type)
trees$Area1[trees$Area1==1]=1 #as.factor(trees$Area1)
trees$Area2[trees$Area2==1]=2 #as.factor(trees$Area1)
trees$Area3[trees$Area3==1]=3 #as.factor(trees$Area1)
trees$Area4[trees$Area4==1]=4 #as.factor(trees$Area1)
trees$Area=as.factor(trees$Area1+trees$Area2+trees$Area3+trees$Area4)
trees[,11:14]<-NULL
# work with a random sample subset of the data to speed things up:
trees.sub <- trees[sample(1:nrow(trees), 1000, replace=FALSE),]
# Problem here: because we are doing subsetting, we end up with empty classes.
# Remove rare groups (n<10) from data:
trees.sub.rm <- trees.sub %>% group_by(Type) %>% filter(n() >= 10)
# expunge empty levels from factor Type
trees.sub.rm$Type=factor(trees.sub.rm$Type)
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
C=10^(-1:5)
G=2^seq(-15,3,2)
ncv<-10 # number of cross-validation folds
# tune SVM using train() under caret
tune_grid <- expand.grid(C = C,sigma=G)
train_control <- trainControl(method = "cv",number=ncv)
system.time(svm_fit<- train(Type~.,data=trees.sub.rm,
method = "svmRadial",   # Choose kernel?
tuneGrid = tune_grid,
#tuneLength = ?,  # number of combinations of tuning parameter(s)
trControl=train_control)) #,epsilon = epsilon
svm_fit
plot(svm_fit)
